{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_fn(u):\n",
    "    \"\"\"Accepts a function u:R^D -> R^D.\"\"\"\n",
    "    J = jacrev(u)\n",
    "    return lambda x: torch.trace(J(x))\n",
    "\n",
    "def output_and_div(vecfield, x):\n",
    "    dx = vecfield(x)\n",
    "    div = vmap(div_fn(vecfield))(x)\n",
    "    return dx, div\n",
    "\n",
    "def normal_logprob(z, mean, log_std):\n",
    "    mean = mean + torch.tensor(0.)\n",
    "    log_std = log_std + torch.tensor(0.)\n",
    "    c = torch.tensor([math.log(2 * math.pi)]).to(z)\n",
    "    inv_sigma = torch.exp(-log_std)\n",
    "    tmp = (z - mean) * inv_sigma\n",
    "    return -0.5 * (tmp * tmp + 2 * log_std + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(nn.Module):\n",
    "\n",
    "    tol = 1e-5\n",
    "\n",
    "    def  __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self._vecfield = nn.Sequential(\n",
    "            nn.Linear(dim + 1, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, dim),\n",
    "        )\n",
    "\n",
    "    def vecfield(self, t, x):\n",
    "        shape = x.shape\n",
    "        x = x.reshape(-1, self.dim)\n",
    "        t = t.reshape(-1, 1).expand(x.shape[0], 1)\n",
    "        tx = torch.cat([t, x], dim=-1)\n",
    "        return self._vecfield(tx).reshape(shape)\n",
    "\n",
    "    def logprob(self, x, t=1.0):\n",
    "        if t == 0:\n",
    "            return normal_logprob(x, 0.0, 0.0).sum(-1)\n",
    "\n",
    "        def combined_CNF_system(t, state):\n",
    "            x, _ = state\n",
    "            dx, div = output_and_div(lambda x: self.vecfield(t, x), x)\n",
    "            return dx, -div\n",
    "\n",
    "        ldj = torch.zeros(x.shape[0]).to(x)\n",
    "        x0, ldj = odeint(combined_CNF_system, (x, ldj), t=torch.linspace(t, 0, 2).to(x), method=\"dopri5\", atol=self.tol, rtol=self.tol)\n",
    "        x0, ldj = x0[-1], ldj[-1]\n",
    "        logp0 = normal_logprob(x0, 0.0, 0.0).sum(-1)\n",
    "        return logp0 - ldj\n",
    "    \n",
    "    def sample(self, x0, t=1.0):\n",
    "        xt = odeint(self.vecfield, x0, t=torch.linspace(0, t, 2).to(x0), method=\"dopri5\", atol=self.tol, rtol=self.tol)\n",
    "        return xt[1]\n",
    "\n",
    "    def sample_with_logprob(self, x0, t=1.0):\n",
    "        def combined_CNF_system(t, state):\n",
    "            x, _ = state\n",
    "            dx, div = output_and_div(lambda x: self.vecfield(t, x), x)\n",
    "            return dx, -div\n",
    "\n",
    "        logprob = normal_logprob(x0, 0.0, 0.0).sum(-1)\n",
    "        xt, logprob = odeint(combined_CNF_system, (x0, logprob), t=torch.linspace(0, t, 2).to(x0), method=\"dopri5\", atol=self.tol, rtol=self.tol)\n",
    "        xt, logprob = xt[-1], logprob[-1]\n",
    "        return xt, logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = (1., 2.)\n",
    "betas = (0.5, -1.0)\n",
    "\n",
    "def T(model, batch_size, Ne):\n",
    "    c = np.pi * np.pi * Ne**3 / 12\n",
    "    x0 = torch.randn(batch_size, 1).to(device)\n",
    "    _, logprob = model.sample_with_logprob(x0)\n",
    "    rho = logprob.exp()\n",
    "    return c * rho * rho\n",
    "\n",
    "def H(model, batch_size, Ne):\n",
    "    c = Ne**2 / 2\n",
    "    x0 = torch.randn(batch_size * 2, 1).to(device)\n",
    "    x = model.sample(x0)\n",
    "    x, xp = x[:batch_size], x[batch_size:]\n",
    "    return c / torch.abs(x - xp)\n",
    "\n",
    "def V(model, batch_size, Ne):\n",
    "    c = Ne\n",
    "    x0 = torch.randn(batch_size, 1).to(device)\n",
    "    x = model.sample(x0).reshape(-1)\n",
    "    summa = 0.0\n",
    "    for alpha, beta in zip(alphas, betas):\n",
    "        summa = summa - alpha * torch.exp(-(x - beta)*(x - beta))\n",
    "    return c * summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_true_results(n_particles: int):\n",
    "    import numpy as onp\n",
    "    d_ = f'Data_1D_GaussMixPot/true_rho_grid_Ne_{n_particles}.txt'\n",
    "    data = onp.loadtxt(d_)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model,ne):\n",
    "    true_data = load_true_results(ne)\n",
    "    xx = torch.linspace(-5, 5, 1000).to(device)\n",
    "    logp = model.logprob(xx.reshape(-1, 1))\n",
    "\n",
    "    p0 = normal_logprob(xx, 0.0, 0.0).exp()\n",
    "\n",
    "    xx = xx.detach().cpu().numpy()\n",
    "    logp = logp.detach().cpu().numpy()\n",
    "    p0 = p0.detach().cpu().numpy()\n",
    "\n",
    "    p_model = np.exp(logp)\n",
    "\n",
    "    dx = (xx[1] - xx[0])\n",
    "    approx_sum = np.sum(dx * p_model)\n",
    "    print(\"integral estimate\", approx_sum)\n",
    "\n",
    "    potential = 0.0\n",
    "    for alpha, beta in zip(alphas, betas):\n",
    "        potential = potential - alpha * np.exp(-(xx - beta)*(xx - beta))\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(xx, p0, color=\"k\", alpha=0.5,label='init CNF')\n",
    "    plt.plot(xx, p_model,label='CNF')\n",
    "    plt.plot(xx, potential, linestyle=\"--\")\n",
    "    plt.plot(true_data[:,0],true_data[:,1],ls=\":\",color='k',label='SCF')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30dd445234e46b7b19624532b4c86a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 00000, loss -1.2873, T 0.0801, H 1.9756, V -1.3675, grad_norm 1.50\n",
      "integral estimate 1.0000278\n",
      "iter 00001, loss -1.4568, T 0.3806, H 284.0678, V -1.8375, grad_norm 1.92\n",
      "integral estimate 1.000014\n",
      "iter 00002, loss -1.4286, T 0.0813, H 2.5836, V -1.5099, grad_norm 1.56\n",
      "integral estimate 0.9999964\n",
      "iter 00003, loss -1.5003, T 0.1811, H 3.4269, V -1.6814, grad_norm 1.17\n",
      "integral estimate 1.0000302\n",
      "iter 00004, loss -1.4005, T 0.2732, H 5.4935, V -1.6737, grad_norm 2.88\n",
      "integral estimate 1.0000185\n",
      "iter 00005, loss -1.4364, T 0.3629, H 7.5019, V -1.7993, grad_norm 2.75\n",
      "integral estimate 1.0000186\n",
      "iter 00006, loss -1.5245, T 0.3352, H 71.1940, V -1.8597, grad_norm 1.16\n",
      "integral estimate 0.9999889\n",
      "iter 00007, loss -1.5431, T 0.1401, H 19.1012, V -1.6833, grad_norm 0.79\n",
      "integral estimate 0.99999666\n",
      "iter 00008, loss -1.4832, T 0.1109, H 2.9737, V -1.5941, grad_norm 0.95\n",
      "integral estimate 0.99998814\n",
      "iter 00009, loss -1.5339, T 0.1325, H 3.0119, V -1.6664, grad_norm 0.62\n",
      "integral estimate 1.0000033\n",
      "iter 00010, loss -1.5811, T 0.2047, H 4.1516, V -1.7858, grad_norm 0.17\n",
      "integral estimate 1.0000215\n",
      "iter 00011, loss -1.5263, T 0.3149, H 5.4485, V -1.8412, grad_norm 0.82\n",
      "integral estimate 1.0000223\n",
      "iter 00012, loss -1.5463, T 0.3043, H 3.7159, V -1.8506, grad_norm 0.73\n",
      "integral estimate 0.9999985\n",
      "iter 00013, loss -1.5476, T 0.2141, H 3.3737, V -1.7617, grad_norm 0.29\n",
      "integral estimate 0.9999775\n",
      "iter 00014, loss -1.5401, T 0.1513, H 5.0433, V -1.6914, grad_norm 0.52\n",
      "integral estimate 0.9999857\n",
      "iter 00015, loss -1.5509, T 0.1491, H 2.2580, V -1.7000, grad_norm 0.62\n",
      "integral estimate 1.0000027\n",
      "iter 00016, loss -1.5635, T 0.1753, H 2.7691, V -1.7389, grad_norm 0.52\n",
      "integral estimate 1.0000187\n",
      "iter 00017, loss -1.5611, T 0.2394, H 3.6610, V -1.8005, grad_norm 0.39\n",
      "integral estimate 1.0000224\n",
      "iter 00018, loss -1.5449, T 0.2814, H 3.8469, V -1.8263, grad_norm 0.69\n",
      "integral estimate 1.0000244\n",
      "iter 00019, loss -1.5577, T 0.2595, H 4.1301, V -1.8172, grad_norm 0.93\n",
      "integral estimate 1.0000203\n",
      "iter 00020, loss -1.5799, T 0.2200, H 3.0885, V -1.7999, grad_norm 0.13\n",
      "integral estimate 1.0000201\n",
      "iter 00021, loss -1.5810, T 0.1844, H 8.3679, V -1.7654, grad_norm 0.60\n",
      "integral estimate 1.0000209\n",
      "iter 00022, loss -1.5734, T 0.1702, H 4.9610, V -1.7437, grad_norm 0.55\n",
      "integral estimate 1.0000194\n",
      "iter 00023, loss -1.5773, T 0.1811, H 7.5752, V -1.7584, grad_norm 0.34\n",
      "integral estimate 1.0000159\n",
      "iter 00024, loss -1.5778, T 0.2117, H 3.8281, V -1.7895, grad_norm 0.27\n",
      "integral estimate 1.0000095\n",
      "iter 00025, loss -1.5664, T 0.2549, H 3.3889, V -1.8212, grad_norm 0.35\n",
      "iter 00026, loss -1.5694, T 0.2650, H 3.9884, V -1.8344, grad_norm 0.46\n",
      "iter 00027, loss -1.5770, T 0.2160, H 2.9206, V -1.7929, grad_norm 0.17\n",
      "iter 00028, loss -1.5539, T 0.1886, H 12.9507, V -1.7424, grad_norm 0.28\n",
      "iter 00029, loss -1.5724, T 0.1890, H 4.8091, V -1.7613, grad_norm 0.27\n",
      "iter 00030, loss -1.5848, T 0.2087, H 7.1900, V -1.7935, grad_norm 0.41\n",
      "iter 00031, loss -1.6124, T 0.2301, H 21.2001, V -1.8426, grad_norm 0.19\n",
      "iter 00032, loss -1.5787, T 0.2228, H 2.9551, V -1.8015, grad_norm 0.56\n",
      "iter 00033, loss -1.5768, T 0.2265, H 3.0692, V -1.8033, grad_norm 0.21\n",
      "iter 00034, loss -1.5861, T 0.2206, H 5.0283, V -1.8067, grad_norm 0.36\n",
      "iter 00035, loss -1.5923, T 0.2086, H 6.7728, V -1.8009, grad_norm 0.42\n",
      "iter 00036, loss -1.5834, T 0.1982, H 5.6194, V -1.7816, grad_norm 0.47\n",
      "iter 00037, loss -1.6110, T 0.1911, H 4.0572, V -1.8021, grad_norm 0.20\n",
      "iter 00038, loss -1.6034, T 0.2166, H 3.8995, V -1.8199, grad_norm 0.40\n",
      "iter 00039, loss -1.5958, T 0.2213, H 4.4901, V -1.8170, grad_norm 0.10\n",
      "iter 00040, loss -1.5898, T 0.2221, H 6.9270, V -1.8119, grad_norm 0.26\n",
      "iter 00041, loss -1.5963, T 0.2016, H 4.1143, V -1.7979, grad_norm 0.31\n",
      "iter 00042, loss -1.5772, T 0.2056, H 3.1990, V -1.7828, grad_norm 0.71\n",
      "iter 00043, loss -1.5940, T 0.2105, H 6.2430, V -1.8045, grad_norm 0.22\n",
      "iter 00044, loss -1.6010, T 0.2240, H 3.1756, V -1.8250, grad_norm 0.50\n",
      "iter 00045, loss -1.6023, T 0.2234, H 5.1786, V -1.8257, grad_norm 0.71\n",
      "iter 00046, loss -1.6067, T 0.2112, H 3.4270, V -1.8179, grad_norm 0.30\n",
      "iter 00047, loss -1.5850, T 0.2006, H 27.4691, V -1.7856, grad_norm 0.44\n",
      "iter 00048, loss -1.5883, T 0.2114, H 4.4862, V -1.7997, grad_norm 0.59\n",
      "iter 00049, loss -1.5867, T 0.2341, H 3.7217, V -1.8209, grad_norm 0.12\n",
      "iter 00050, loss -1.5845, T 0.2402, H 9.4087, V -1.8248, grad_norm 0.71\n",
      "integral estimate 1.0000104\n",
      "iter 00051, loss -1.6037, T 0.2052, H 3.5337, V -1.8089, grad_norm 0.72\n",
      "iter 00052, loss -1.5957, T 0.1945, H 3.8833, V -1.7902, grad_norm 0.16\n",
      "iter 00053, loss -1.5960, T 0.2034, H 3.0549, V -1.7994, grad_norm 0.96\n",
      "iter 00054, loss -1.5992, T 0.2034, H 6.1347, V -1.8025, grad_norm 0.63\n",
      "iter 00055, loss -1.6019, T 0.2165, H 2.8030, V -1.8184, grad_norm 0.40\n",
      "iter 00056, loss -1.5997, T 0.2243, H 4.3059, V -1.8241, grad_norm 0.66\n",
      "iter 00057, loss -1.5928, T 0.2117, H 4.0811, V -1.8045, grad_norm 0.14\n",
      "iter 00058, loss -1.5869, T 0.2066, H 4.7808, V -1.7934, grad_norm 0.44\n",
      "iter 00059, loss -1.5972, T 0.2038, H 6.4696, V -1.8010, grad_norm 0.54\n",
      "iter 00060, loss -1.5696, T 0.2226, H 3.0167, V -1.7922, grad_norm 0.81\n",
      "iter 00061, loss -1.5745, T 0.2263, H 3.7077, V -1.8008, grad_norm 0.32\n",
      "iter 00062, loss -1.5756, T 0.2164, H 3.6764, V -1.7919, grad_norm 0.55\n",
      "iter 00063, loss -1.5931, T 0.2131, H 5.9058, V -1.8061, grad_norm 0.45\n",
      "iter 00064, loss -1.5850, T 0.2237, H 12.8220, V -1.8087, grad_norm 0.49\n",
      "iter 00065, loss -1.6056, T 0.2175, H 3.5347, V -1.8231, grad_norm 0.33\n",
      "iter 00066, loss -1.5866, T 0.1990, H 4.8953, V -1.7855, grad_norm 0.22\n",
      "iter 00067, loss -1.5796, T 0.1933, H 3.1685, V -1.7729, grad_norm 0.55\n",
      "iter 00068, loss -1.5910, T 0.2275, H 3.9756, V -1.8184, grad_norm 0.18\n",
      "iter 00069, loss -1.6003, T 0.2293, H 3.5770, V -1.8297, grad_norm 0.50\n",
      "iter 00070, loss -1.6057, T 0.2084, H 7.5183, V -1.8141, grad_norm 0.23\n",
      "iter 00071, loss -1.5857, T 0.1991, H 4.2452, V -1.7849, grad_norm 0.69\n",
      "iter 00072, loss -1.6061, T 0.2057, H 5.5154, V -1.8118, grad_norm 0.19\n",
      "iter 00073, loss -1.5744, T 0.2177, H 5.3128, V -1.7920, grad_norm 0.62\n",
      "iter 00074, loss -1.5994, T 0.2324, H 4.1783, V -1.8318, grad_norm 0.19\n",
      "iter 00075, loss -1.5896, T 0.2248, H 3.2280, V -1.8144, grad_norm 0.38\n",
      "iter 00076, loss -1.5901, T 0.2175, H 3.2554, V -1.8075, grad_norm 0.23\n",
      "iter 00077, loss -1.5986, T 0.1999, H 3.6850, V -1.7985, grad_norm 0.35\n",
      "iter 00078, loss -1.5883, T 0.2006, H 2.8372, V -1.7890, grad_norm 0.21\n",
      "iter 00079, loss -1.5915, T 0.2169, H 6.9274, V -1.8084, grad_norm 0.33\n",
      "iter 00080, loss -1.5999, T 0.2229, H 8.8852, V -1.8228, grad_norm 0.14\n",
      "iter 00081, loss -1.5980, T 0.2213, H 3.5182, V -1.8193, grad_norm 0.16\n",
      "iter 00082, loss -1.6005, T 0.2087, H 4.1453, V -1.8092, grad_norm 0.04\n",
      "iter 00083, loss -1.5928, T 0.1982, H 9.4885, V -1.7910, grad_norm 0.37\n",
      "iter 00084, loss -1.5930, T 0.2049, H 3.7715, V -1.7979, grad_norm 0.36\n",
      "iter 00085, loss -1.6062, T 0.2251, H 4.0535, V -1.8312, grad_norm 0.12\n",
      "iter 00086, loss -1.5940, T 0.2287, H 4.1697, V -1.8227, grad_norm 0.21\n",
      "iter 00087, loss -1.5948, T 0.2136, H 3.1644, V -1.8084, grad_norm 0.04\n",
      "iter 00088, loss -1.6006, T 0.2068, H 3.3523, V -1.8074, grad_norm 0.17\n",
      "iter 00089, loss -1.5729, T 0.2009, H 10.1074, V -1.7738, grad_norm 0.17\n",
      "iter 00090, loss -1.5833, T 0.2132, H 3.4071, V -1.7966, grad_norm 0.12\n",
      "iter 00091, loss -1.5942, T 0.2359, H 5.0647, V -1.8301, grad_norm 0.15\n",
      "iter 00092, loss -1.6071, T 0.2329, H 3.8706, V -1.8400, grad_norm 0.18\n",
      "iter 00093, loss -1.6066, T 0.2158, H 81.9843, V -1.8224, grad_norm 0.18\n",
      "iter 00094, loss -1.6049, T 0.1899, H 3.3591, V -1.7949, grad_norm 0.18\n",
      "iter 00095, loss -1.5728, T 0.2023, H 3.9228, V -1.7751, grad_norm 0.29\n",
      "iter 00096, loss -1.5955, T 0.2085, H 4.6636, V -1.8040, grad_norm 0.18\n",
      "iter 00097, loss -1.6044, T 0.2209, H 2.7356, V -1.8253, grad_norm 0.22\n",
      "iter 00098, loss -1.5930, T 0.2145, H 5.6735, V -1.8074, grad_norm 0.56\n",
      "iter 00099, loss -1.5955, T 0.2165, H 6.5630, V -1.8120, grad_norm 0.82\n",
      "iter 00100, loss -1.5899, T 0.2061, H 3.7939, V -1.7960, grad_norm 0.23\n",
      "integral estimate 1.0000176\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "os.makedirs(\"ckpts\", exist_ok=True)\n",
    "\n",
    "Ne = 1\n",
    "batch_size=1024\n",
    "grad_clip=1.0\n",
    "\n",
    "model = CNF(1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "running_loss = RunningAverageMeter()\n",
    "running_T = RunningAverageMeter()\n",
    "running_H = RunningAverageMeter()\n",
    "running_V = RunningAverageMeter()\n",
    "\n",
    "for itr in tqdm(range(101)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # H_coef = min(itr / 2000, 1.0)\n",
    "    H_coef = 0.0\n",
    "    \n",
    "    T_val = T(model, batch_size, Ne)\n",
    "    H_val = H(model, batch_size, Ne)\n",
    "    V_val = V(model, batch_size, Ne)\n",
    "\n",
    "    loss = T_val + H_coef * H_val + V_val\n",
    "    loss = loss.mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss.update(loss.item())\n",
    "    running_T.update(T_val.mean().item())\n",
    "    running_H.update(H_val.mean().item())\n",
    "    running_V.update(V_val.mean().item())\n",
    "    \n",
    "    print(f\"iter {itr:05d}, loss {running_loss.val:.4f}, T {running_T.val:.4f}, H {running_H.val:.4f}, V {running_V.val:.4f}, grad_norm {grad_norm:.2f}\")\n",
    "\n",
    "    if itr % 10 == 0 or itr < 25:\n",
    "\n",
    "        plot(model,Ne)\n",
    "        plt.savefig(f\"figs/{itr:06d}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    if itr % 500 == 0:\n",
    "        torch.save(model.state_dict(), f\"ckpts/ckpt-N={Ne}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
